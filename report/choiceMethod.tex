\section{Techniques for Author Attribution}
\label{choiceMethod}
In this section I discuss the existing major types of Author Attribution, and debate the advantages and disadvantages of each with respect to this project.

To give an example of the different techniques, I will use a text extract from ``The Federalist Papers'' \cite{federalist} unless it is not applicative. It should be noted that each chosen method is merely an example for that category and should seen as nothing more.

I will use the following criteria in order to evaluate the effectiveness of the method:
\begin{itemize}
\item It should work on reasonable amount of text
\item The time complexity of the algorithm should be reasonable and it should scale well
\item I should be practically possible to implement the solution without using an unreasonable amount of time
\item I should not be an expert in natural language processing in order to implement the solution
\item The solution should work on languages using a character alphabet, though it would be a plus if it could also be used on pictogram based alphabets
\item The algorithm should need as little manual intervention as possible --- i.e. I should preferably only have to implement an algorithm --- not setup large databases with rules and language specific probabilities.  
\end{itemize}

\subsection*{Example text}
The example text used will be:\\
\q{His proposition is, "that whenever any two of the three branches of government shall concur in opinion, each by the voices of two thirds of their whole number, that a convention is necessary for altering the constitution, or CORRECTING BREACHES OF IT, a convention shall be called for the purpose.}
From \cite{federalist}

\method{Lexical}
{\label{character}
Lexical analysis splits the text into tokes which then can be analysed. I will here focus on $n$-gram analysis. $N$-gram analysis is done by taking a text, and breaking it into all possible permutations of n-consecutive tokens called ``grams''. Once all the $n$-grams for a text have been computed, they are used in a statistical analysis with the corpora of the possible authors. 
}
{
In this report I have chosen to represent whitespace by the $\_$ character: Since any reasonable large text would create a very large number of 3-grams, I have chosen, unlike all the other examples, to use only the part before the first comma.:\\
\q{His proposition is,}
which would create the following 3-grams: \ngr{His}, \ngr{is\_}, \ngr{s\_p}, \ngr{\_pr}, \ngr{pro}, \ngr{rop}, \ngr{opo}, \ngr{pos}, \ngr{osi}, \ngr{sit}, \ngr{iti}, \ngr{tio}, \ngr{ion}, \ngr{on\_}, \ngr{n\_i}, \ngr{\_is}, \ngr{is,}.
which then would be used with a statistical model 
}
{
\item Does not require specialised tools or much in the way prepossessing
\item Gives very good results in practice --- see \cite{nr4} and \cite{nr3}.
\item The text does not need to be spelled correctly, as long as words are spelled consistently.
\item Ignores problems with word or sentence boundaries.
\item Might be applicable to pictogram based alphabets.  
}
{
\item Can create very large data sets, since trying to create all $n$-grams on a text with m characters (given $n < m$) will result in $m - n + 1$ $n$-grams --- each n characters --- resulting in $n * (m - n + 1) = n*m - n^2$ + n characters.
\item The statistical method might be very complex.
\item Since it is a statistical method, it might require a lot of data.
}

\method{Syntactic Features}
{\label{syntactic}
These types of methods tries to identify the author through the syntactic features that has been added subconsciously by the authors. Syntactic features include the frequency of different types of phrases (for instance \cite{style} uses concepts such as noun phrases, verb phrases, prepositional phrase, adverbial phrase and conjunctions\footnote{A naunphrase is a phrase that centers around a noun, while a verb phrase is a phrase that centers around a verb, and so on. A conjunction is a part of text that bridges two different parts, but has little meaning in itself}) and the frequency of punctuation.
} 
{
In the follow NP defines a Noun Phrase, VP a Verb Phrase, PP prepositional phrase, ADVP a adverbial phrase and CON conjunction.\\
\q{\ann{NP}{His proposition is}, "\ann{CON}{that whenever any two of \ann{NP}{the three branches of government} \ann{VP}{shall concur in opinion}, \ann{NP}{each by the voices of two thirds of their whole number}, \ann{NP}{that a convention is} \ann{VP}{necessary for altering the constitution},\ann{CON}{or CORRECTING BREACHES OF IT}, \ann{VP}{a convention shall be called for the purpose.}}
}
}
{
\item Could be very accurate.
}{
\item Requires advanced software that can identify the different parts of the sentence --- which is beyond the scope of this project --- or a lot of manual intervention.
}

\method{Semantic information}
{\label{stylistic}
Looking at the style of a text seems like an obvious choice when trying to identify the author (and indeed, not only the name, but also other data, like age and sex). In order to do, the text must be parsed to identify and mark parts of the text for certain predefined categories. \cite{style} mentions 3 top categories: 
\begin{description}
\item[Cohesion:] How a text is constructed. Cohesion is constructed out of  ``Elaboration'' (when something is explained in greater detail), ``Extension'' (when new information is added) and ``Enhancement'' (greater qualification within the context).
\item[Assessment:] How a text \q{constructs propositions as statements of belief, obligation, or necessity}\footnote{\cite{style}, p. 804} --- constructed out of  ``Type'' (modality), ``Value'' (the degree of modality), ``Orientation'' (relation to the modality), ``Manifestation'' (relation of the modal assignment to the event), each with further subcategories, all hung on certain words.
\item[Appraisal:] The expression of an attitude towards an object.
\end{description}
}
{
\q{His proposition is, "that \ann{Value}{whenever} any two of the three branches of government shall concur in opinion, \ann{Elaboration}{each by the voices of two thirds of their whole number}, \ann{Orientation}{that a convention is necessary for altering the constitution, or CORRECTING BREACHES OF IT}, a convention \ann{Type}{shall} be called for the purpose.}
}  
{
\item Invariant to word or phrase boundaries.
\item Given that the system could differentiate between the different cases, it is likely that information about the text could be used to construct a profile.
}{
\item For optimal efficiency it would require the entire text to be spelled correctly. Since I intend to create my corpus from text from Internet forums, this requirement cannot be satisfied.
\item The system seems to require that the specific words must be identified and tied to categories. This would mean that it cannot be applied to another language, e.g. danish,  without a prohibitively amount of work that I cannot see how should be automated.
} 

\method{Application Specific}
{\label{application}
Instead of fitting a general model on every text, the application specific takes care to use information about the medium in question --- such as HTML tags (for texts found on the Internet), indentation, the use of signature, fonts, size etc. All tailored to the specific applications.
}
{
Since this method is primarily used for texts that appears in either computer programs or the Internet (forum, blog or e-mail) I cannot use the example from the Federalist papers. Instead I will use the following e-mail, using \cite{Vel01mininge-mail} to analyse it: 

\q{Hej, Jeg har provet at folge dit raad, og har provet at stramme op i test afsnittet. Mvh. Mikkel} 

Which then would be analysed and mined for 170 different values. Two examples would be
\begin{description}
\item[Number of blank lines / number of lines:] 0 / 1
\item[Average sentence length:] 42 letters
\end{description}
}{
\item Could be very fitting since this project focuses explicitly on posts from Internet forums, and that different users have widely different ways of making their posts (i.e. the length, the inclusion of HTML tags, signatures etc.).
}{
\item The final solution might end up being too specific, and would thus only work on a subsection of forums or wikis, which uses the same formatting as the algorithm looks for.
\item Requires more advanced pattern recognition than could be implemented in this project. 
}

\subsection{Conclusion}
\label{technique:conclusion}
From this I have concluded that the best choice would be to use the character based $n$-gram approach - since this approach is both simpler and less labour-intense than some of the other attribution models, while at the same time yielding good results. Since the data I have is in Danish, I will not try to accommodate pictogram based alphabets --- e.g. Chinese.


\subsection{Choice of $n$-gram analysis}

\subsubsection*{$N$-gram Feature Selection for Authorship Identification}
The solution proposed in \cite{nr3} first selects the most important $n$-grams, using a number of rules. In order to compare whether or not a certain author has written a certain text, another involved formula is then used, to gauge the information that can be gained from the author's corpus.

I find this $n$-gram analysis intimidating --- not only does the functions themselves seem rather advanced, but it is potentially even more complicated, as not all the key concepts are not explained in the paper. It also seems that the key ``glue'' function requires manual intervention, one of the things I was trying to avoid. 

\subsubsection*{$N$-gram method based on Bennet}
The paper \cite{nr2} uses a improved version of the algorithm found in \cite{Bennet}. Bennet uses the basic idea that an authors profile can be found by calculating the difference between the frequency of a character in the English language (based on some standard frequency) and the frequency of the author.\\

The Authors of \cite{nr2}, however, use the following formula
$$
\sum_{n \in \rm{\emph{profile}}}\left(\frac{2 \cdot (f_1(n) - f_2(n))}{f_1(n) + f_2(n)}\right)^2
$$
which calculates the dissimilarity between the text t and one of the potential authors. $f_1(n)$ and $f_2(n)$ is the frequency of $n$-gram n in the document and the corpus of the author, respectively. 

This method has the clear advantage that it is rather straightforward, and unlike \cite{Bennet} it does not require standard frequencies for the English language (which there could be multiple versions off --- and results, depending on the version used). One might wonder whether or not this system is too simple.

\subsubsection*{Language Independent Authorship Attribution using Character Level Language Models}
The method found in \cite{nr4} is  based on statistical analysis, which implements its own algorithm to rate the similarity of the different authors. The underlying assumption is that the chance of the i'th gram being something is a function being based on all the i - 1 preceding grams. Since it is practically unfeasibly to base it on the i - 1 preceding grams, the implemented method only looks on the last n grams, where $n \in \{3,4,5,6 \}$. 

\subsection{Final choice}

In the end I choose Language Independent Authorship Attribution using Character Level Language Models for several reasons:
\begin{itemize}
\item The algorithm looked like it could be implemented without too much work
\item The authors had reported good results on the English language, as well as Chinese and Greek, so I presume it it would also give good results for Danish.
\item The good results had been reported on famous Authors with a very specific way of writing --- and the Authors themselves note that it would be interesting to apply their methods on less distinct corpora. 
\end{itemize}

I furthermore choose to set $n = 3$, on the grounds that it will make the amount of data that must be processed manageable (since, as mentioned earlier the process will need to remember n*m - $n^2$ + n characters.). Since I in the implementation phase found that it also was prudent to calculate all the preceding $n$-grams (in this case 1- and 2-grams), this was a good choice.
