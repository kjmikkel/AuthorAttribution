\section{Method used}
\label{method}

\subsection{Introduction}
In this section I will detail the method I have chosen to use for Author Attribution. I will explain how and why it works, as well as introducing Good-Turing smoothing, which needed for the method to work.

\subsection*{Notation}
In the following I will use $c_i$ to denote the i'th character in the text.

\subsection{The method}
The method in \cite{nr4} tries to find the author of a given text \texttt{t}. It does so by calculating the probability of each author having written \texttt{t}, and then chooses the one with the greatest probability.

My implementation differs from the one in \cite{nr4} since it uses characters as the basic unit, instead of the words. 

The idea behind the method is that in any text probability of character $c_i$ having a certain value, is based on the previous characters, in order to make meaningful word and sentences. Each author has of course a his own way of writing, and so will use place the letters in his own way. Since the way that the author writes is unique, so will the distribution of n-grams, which then can be used for Authorship Attribution.

\subsubsection*{The Algorithm}
In this section I will describe the algorithm used to find the probability of an author having written the text.\\

In order to find the most probable author, the following algorithm is executed:

\begin{enumerate}
\item All the n-grams for the text \texttt{t} from the unknown author is calculated
\item All the n-grams for each author in the corpus is calculated
\item We check each n-gram from the unknown author against each author in the corpora with the Pr algorithm in (\ref{eq:probNorm}).  
\item The author with the highest probability is attributed as the author of \texttt{t}.
\end{enumerate}

\begin{equation}
\label{eq:probNorm}
Pr(c_{i - n + 1} \ldots c_{i}) = \left\{
\begin{array}{rl}
\hat{P}r(c_{i - n + 1} \ldots c_{i}), \text{if } \#(c_{i - n + 1} \ldots c_{i}) > 0\\
\beta(c_{i - n + 1} \ldots c_{i-1}) \cdot Pr(c_{i - n + 2} \ldots c_{i}), \mathrm{otherwise}\\
0, \mathrm{if} c_{i-n+1}\ldots c_i \mathrm{is\ the\ empty\ string}
\end{array} \right.
\end{equation}

where 
\begin{equation}
\label{eq:probHat}
\hat{P}r(c_{i - n + 1} \ldots c_{i}) = \frac{discount \#(c_{i - n + 1} \ldots c_{i})}{\#(c_{i - n + 1} \ldots c_{i-1})}
\end{equation}

\begin{equation}
\label{eq:beta}
\beta(c_{i - n + 1} \ldots c_{i-1}) = 
\frac
{1 - \sum_{x \in (c_{i - n + 1} \ldots c_{i-1})}\hat{P}r(c_{i - n + 1} \ldots c_{i-1} x)}
{1 - \sum_{x \in (c_{i - n + 1} \ldots c_{i-1})}\hat{P}r(c_{i - n + 2} \ldots c_{i-1} x)}
\end{equation}.

In (\ref{eq:probNorm}) there are 3 cases:
\begin{enumerate}
\item A n-gram appears in the corpus - so it is meaningful to ask about the probability of it being in the corpus.
\item  A n-gram does not appear, and we then attempt to calculate the probability of having the (n-1)-gram appearing in the corpus. To ensure that the value is correct, we need to multiply with the normalization constant $\beta$. 
\item In case we have to test whether the empty n-gram appears, I have decided that the algorithm should return 0. This is not covered in \cite{nr4}. My justification is that, since the empty n-gram contains no information and should therefore return 0.
\end{enumerate}
In (\ref{eq:probHat}) $discount\#(c_{i - n + 1} \ldots c_{i})$ 
is the discounted probability, calculated by using Good-Turing smoothing on the corpora of the author we are testing against. The discounted smoothing is applied to give a positive probability for n-grams not in the corpora. For more information on Good-Turing smoothing, see section \ref{Good-Turing}.\\

The reason for the division in (\ref{eq:probHat}) is that since the probability of the i'th character appearing is dependent on the former n-1 character, its probability is the number of times it appears in the corpus, divided by the number of times last n - 1 characters leading up to it.\\

$\beta (w_{i - n + 1}\ldots w_{i -1})$ in (\ref{eq:beta}) is the normalization constant, needed to ensure that the area under the graph is 1. The version seen here is an interpretation of the one found in \cite{nr4}, as the version in \cite{nr4} contained errors. I have tried on multiple occasions to contact the authors of \cite{nr4}, but I have had no response, and I have therefore tried to interpret the results myself.
\begin{description}
\item[$\hat{P}$:]  In \cite{nr4} $\hat{P}$ is found  both in the dividend and the divisor of (\ref{eq:beta}). Since there is no other mention of  $\hat{P}$ in \cite{nr4}, I have assumed that it is a typo and that they meant $\hat{P}r$ in (\ref{eq:beta}).
\item[Wayward x's:] In (\ref{eq:beta}) there is also the $\sum_{x \in (c_{i - n + 1} \ldots c_{i-1} x)}$, which I have interpreted as $\sum_{x \in (c_{i - n + 1} \ldots c_{i-1})}$.  
\end{description}

\subsection{Good-Turing smoothing}
\label{Good-Turing}
Good-Turing smoothing is a statistical method to find out the probability of whether a given object will appear, given a collection of previous objects. In this case, given that we have observed the n-grams up to a certain point, what is the chance that n-gram x is observed? It is essential that the smoothing technique ensures that n-grams not yet observed (one of the so-called unseen objects), has a positive probability\footnote{The reasoning for this is that, while the n-grams has never appeared, it is unlikely that the author has never used it at all - just never in the given corpora}, and that the probability of all seen n-grams adds up to 1.\\

In order to calculate the probability of each n-gram, the Good-Turing algorithm calculates the frequency (denoted by $N_r$)of all frequencies (denoted r) of n-grams. Let us assume that we want to apply Good-Turing smoothing on the following text: ``abcdabceabc c''. First we calculate the n-grams of the text:\\
$$
\nga{abc}, \nga{bcd}, \nga{cda}, \nga{dab}, \nga{abc}, \nga{bce}, \nga{cea}, \nga{eab}, \nga{abc}, \nga{bc\_}, \nga{c\_c}
$$
which gives us the table\\
\begin{tabular}{|cc|}
\hline
Frequency & Frequency of frequency \\
\hline
r & $N_{r}$ \\
1 & 8\\
3 & 1\\
\hline
\end{tabular}\\
Since there is 1 n-gram ($\nga{abc}$) which appears 3 times, while there is 8 n-grams which only appears 1 time. 
\\\\
We can then calculate the probability for all the unseen n-grams. We first have to calculate N, where 
$$
N = \sum r * N_r
$$
so in this case N = 1 * 8 + 3 * 1 = 11. The frequency of the n-grams that does not appear is then given by 
$$
\mathrm{Unseen} = \frac{N_1}{N} = \frac{8}{11} = 0.73
$$

The probability of having a n-gram that appears r times is then 
$$p_r = \frac{r^*}{N}$$
 where 
$$r^* = (r+1)\frac{E(N_{r+1})}{E(N_r)}$$
 where E(x) is the expectation of x appearing. So in this case we have that 

$$p_1 = \frac{(1 + 1) * \frac{3}{8}}{11} = \frac{\frac{6}{8}}{11} = \frac{6}{88} = \frac{3}{44}$$ and  
$$p_3 = \frac{(3 + 1) * \frac{0}{1}}{11} = 0$$

The most interesting of these two is of course $p_3$, which has returned 0, even though we intuitively would think it returned a positive number.

Many of the recent implementation of Good-Turing smoothing comes from the work done in \cite{Gale94good-turingsmoothing}, which also describes the algorithm in greater detail.
