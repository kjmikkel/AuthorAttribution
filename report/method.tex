\section{Method used}
\label{method}

\subsection{Introduction}
In this section I will explain the method I used for author attribution - and how and why it works.

\subsection{The method}
\fixme{Make a proper introduction}
The method in \cite{nr4} tries to find the author of a given text \texttt{t}. It does so by calculating the probability of each author having written \texttt{t}, and then chooses the one with the greatest probability.

My implementation differs from the one in \cite{nr4} since it uses characters as the basic unit, instead of the words.

In the following $c_m$ is the m'th character in the text. As stated before \fixme{stated where?}, the probability of the value of i'th character is based on the past (i - 1) characters (with the more recent characters counting for more \fixme{is this true}). However, calculating this probability would be prohibitory expensive. With n-grams, we only use the probability for the last n characters - so the interesting characters would be $c_{i - n + 1} \ldots c_{i}$.

\begin{equation}
\label{eq:probNorm}
Pr(c_{i - n + 1} \ldots c_{i}) = \left\{
\begin{array}{rl}
\hat{P}r(c_{i - n + 1} \ldots c_{i}), \text{if } \#(c_{i - n + 1} \ldots c_{i}) > 0\\
\beta(c_{i - n + 1} \ldots c_{i-1}) \cdot Pr(c_{i - n + 2} \ldots c_{i}), \mathrm{otherwise}\\
0, \mathrm{if\ no\ consequently\ subsection\ appears\ in\ the\ corpus}
\end{array} \right.
\end{equation}

where 
\begin{equation}
\label{eq:probHat}
\hat{P}r(c_{i - n + 1} \ldots c_{i}) = \frac{discount \#(c_{i - n + 1} \ldots c_{i})}{\#(c_{i - n + 1} \ldots c_{i-1})}
\end{equation}

\begin{equation}
\label{eq:beta}
\beta(c_{i - n + 1} \ldots c_{i-1}) = 
\frac
{1 - \sum_{x \in (c_{i - n + 1} \ldots c_{i-1})}\hat{P}r(c_{i - n + 1} \ldots c_{i-1} x)}
{1 - \sum_{x \in (c_{i - n + 1} \ldots c_{i-1})}\hat{P}r(c_{i - n + 2} \ldots c_{i-1} x)}
\end{equation}.

In (\ref{eq:probNorm}) there are 3 cases:
\begin{enumerate}
\item A n-gram appears in the corpus, which means it is meaningful to ask abouts it probability of it appearing the corpus.
\item  A n-gram does not appear, and we then attempt to calculate the probability of having the (n-1)-gram appearing in the corpus. To ensure that the value is correct, we need to multiply with the normalization constant $\beta$. 
\item In case we have to test whether the empty n-gram appears, I have decided that the algorithm should return 0. This is not covered in \cite{nr4}. The justification for this is that, since the empty n-gram does not appear in the corpus, then its probability must be 0
\end{enumerate}
In ( \ref{eq:probHat} ) $discount\#(c_{i - n + 1} \ldots c_{i})$ 
is the discounted probability, calculated by using Good-Turing smoothing\footnote{Which the report mentions gives good results in practice, as well as a fair speed \cite{nr4}. For more information on Good-Turing, see \ref{Good-Turing}} on the corpora of the author we are testing against. The discounted smoothing is done to remove some of the n-grams that appears infrequently, and thus has no real significance for the corpora. This was especially relevant when the n-grams consist of words, as it did in (\cite{nr4}), since the alphabet of words in the English language than the alphabet of letters. \fixme{Decide whether it is worth explaining the division.}. The division in \ref{eq:probHat} of the number of occurrences of $c_{i - n + 1} \ldots c_{i - 1}$ in the corpora is done . Since the probability of the i'th character appearing is dependent on the former n-1 character leading up to it, its probability must its chance of appearing in the corpora, divided by the number of times the build up appears \fixme{Describe this better}.
$\beta (w_{i - n + 1}\ldots w_{i -1})$ in (\ref{eq:beta}) is the normalization constant, needed to ensure that the area under the graph is 1. It should be mentioned that the $\hat{P}r$ in both the dividend and the divisor is $\hat{P}$ in \cite{nr4}, but since this particular function is not mentioned anywhere in the text, I have assumed that it is $\hat{P}r$.


\subsection{Good-Turing smoothing}
\label{Good-Turing}
Good-Turing smoothing is a statistical method to find out the probability of whether a given object will appear, given a collection of previous objects. In this case, given that we have observed the n-grams up to a certain point, what is the chance that n-gram x is observed? It is essential that the smoothing technique ensures that n-grams not yet observed, has a non-zero chance of appearing, and that the probability of all n-grams add up to 1.\\

In order to calculate the probability of each n-gram (not only the ones not appearing), the Good-Turing algorithm calculates the frequency (denoted by $N_r$)of all frequencies (denoted r). So for instance, there are precisely 3 n-gram that appears once - then the frequency of n-grams that appears once would be 3. For example, let us assume we find the n-grams of the text ``abcdabceabc c'' have the following set of n-grams\footnote{We do not use the n-grams calculated in \ref{character}, as all of those only appears once, and thus would not be very illustrative for the method}:\\
$$
\{\nga{XXa}, \nga{Xab}, \nga{abc}, \nga{bcd}, \nga{cda}, \nga{dab}, \nga{abc}, \nga{bce}, \nga{cea}, \nga{eab}, \nga{abc}, \nga{bc\_}, \nga{c\_c}, \nga{\_cX}, \nga{cXX} \}
$$
which gives us the table\\
\begin{tabular}{|cc|}
\hline
Frequency & Frequency of frequency \\
\hline
r & $N_{r}$ \\
1 & 12\\
3 & 1\\
\hline
\end{tabular}
\\
We can then calculate the probability for all those n-grams that does not appear in the text. We first have to caculdate N, where 
$$
N = \sum r * N_r
$$
so in this case N = 1 * 12 + 3 * 1 = 15. The frequency of the n-grams that does not appear is then given by 
$$
\frac{N_1}{N} = \frac{12}{15} = 0.8
$$

The probability of having the a n-gram that appears r times is then 
$$p_r = r^*/N$$
 where 
$$r^* = (r+1)\frac{E(N_{r+1})}{E(N_r)}$$
 where E(x) is the expectation of x appearing. So in this case 
$$p_1 = \frac{(1 + 1) * \frac{3}{12}}{15} = \frac{\frac{1}{2}}{15} = \frac{1}{30}$$ and  
$$p_3 = \frac{(3 + 1) * \frac{0}{1}}{15} = 0$$
\fixme{This cannot be right}
Many of the recent implementation of Good-Turing smoothing comes from the work done in \cite{Gale94good-turingsmoothing}, which also describes the algorithm in greater detail.

\subsection{Implementation changes}
Through experimentation, I have found that it essential that if one has to test for n-grams of length 3, the n-grams of length 1, 2 and 3 has to be made, due to the way that Good-Turing Smoothing works.

\subsection{Architecture}
\fixme{add image of architecture}
