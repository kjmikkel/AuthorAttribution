\section{Implementation}
\label{implementation}

In this section I will try to describe some of the more interesting implementation choices I have made during this project.

\subsection{Choice of programming language}
I choose to write my project in Python, on the grounds that there was no need for a object-oriented model (which excluded both Java and C\#), nor was there a bigger need for speed than development time (which removed both C and C++). Furthermore I did not feel that there was any special need to use a functional language, and since I felt more comfortable with Python - I ended up using that.

\subsection{Creation of n-grams}
Since the creation of n-grams themselves was not a critical part of the project, I opted not to write my own code (which would have meant that I should have taken care of a lot of possible edge conditions). Therefore I choose python-ngram (version 2.0b2) which can be found at http://python-ngram.sourceforge.net/\footnote{Address has been tested to work on the 13th of August, 2009}. To make it fit the program better I have altered some of the code, and even added my own. There is no overarching pattern - but I have clearly labeled the changes in the code itself.
\subsubsection{Numbers}
Since numbers are bound to appear in a text, I have had to take care of how to present them. Since there is little chance of the same number appearing again and again, and since the usage of numbers themselves are more interesting, I have opted to turn all numbers into the same escape character: ``$\backslash$ v''
\subsubsection{Padding}
The code would originally add padding to the start and the end of the text. This was most likely done to ensure that the (n-1) first and the (n-1) last characters would have their own n-grams. However, since I also create 1 and 2 grams (see below), I do not find any use for this feature, and since the padding was made by plain capital X's, I choose to un-comment the part of the code.

\subsection{Good-Turing smoothing}
Like above, I did not choose to write my own Good-Turing smoothing, but did rather use the ``Natural Language Toolkit'' for python, version 0.9.9, which can be found at www.nltk.org\footnote{Address has been tested to work on 13th of August, 2009}.

\subsection{Implementation changes}
Through experimentation, I have found that it essential that if one has to test for n-grams of length 3, the n-grams of length 1, 2 and 3 has to be made, in order to get a probability that is greater than 0. If this correction is not done, the $N_{r+1}$ that caused $p_3$ in \pref{Good-Turing} to become 0 will likewise make all the results 0. However, there is another advantage of making both the 1- and 2-grams. There is a part of the algorithm that counts the number of occurrences of the gram in the text. If I use the built-in python count function on the text, I might end up with a smaller number due to its implementation. For instance, if I had the text ``sss'' and I had to check for the text ``ss'', I would get the value 1, while if I used the n-gram solution I would get 2 - which I find to be the correct value.

\subsection{Scalability of implementation}
In order to measure whether or not the algorithm scales, I have found it necessary to make 2 tests: 
\begin{enumerate}
\item The time it takes to make the n-grams from the corpora. The only parameter in this case is the size of the corpora we will create n-grams for.
\item The time it takes to decide which author is most likely to have written a text. This test has 2 parameters: The size of the corpora to test against, and the number of test to make
\end{enumerate}

In order to get a proper resolution for both tests, I have chosen 12 different corpora and text sizes, and will test each and every combination. The sizes for both the corpora and the texts are as follows 100, 200, 300, $\ldots$, 1100, 1200.

\subsubsection{Make n-gram}
\input{/home/mikkel/Documents/AuthorAttribution/report/tabeller/ngramTime}

\subsubsection{Decide authors}
\input{/home/mikkel/Documents/AuthorAttribution/report/tabeller/crossSave}
