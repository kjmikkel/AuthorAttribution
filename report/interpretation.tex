\section{Interpretation}
\label{interpretation}
In this section I will interpreter the different test results presented in the last section. I will do so test by test, before giving a conclusion of all the tests.

\subsection{StressTest}
The test results in the last section clearly shows that not one of StressTests has a single hit. While not surprising, it does however mean that that the algorithm is unlikely to be able to identify anonymous posts (since multiple anonymous posts may be written by different authors).

\subsection{AuthorSomePost}
We see that in both 1 and 3 there is a single correct hit

\subsection{AuthorManyPost}
This test is an interesting case, since all of the sub tests have correct hits, unlike all other tests. While it would be fair to presume that this is because each author has more posts, it is worth noting that both the first and second test has a poorer score on all 4 dimensions (recall, precision, Overall Accuracy and Macro-average F-measure), than the third test, which has the fewest posts - \pref{reportTable}. From the same source it also becomes apparent that A1's post on average are shorter than A3's. This illustrates that more, and longer posts does not trivially make the algorithm better, but that the contents (and the n-grams found within), must play a large part in it.

\subsection{ShortBogusTest}
It is clear from the table that the algorithm consistently has chosen the bogus texts, which might be an indicator of that the algorithm weights corpora with few posts unfairly compared to those with longer, and that for it to work properly, there should be a more even distribution of posts from each author.\\

One possible explanation might lie in the way that Good-Turing calculates the value for all the unseen object (the n-grams that does not appear in the authors corpora). As previously stated in \ref{Good-Turing} (p. \pageref{Good-Turing}) the probability of an unseen object in the Good-Turing smoothing is $u =\frac{N_1}{N}$, where $N_1$ is the frequency of the frequency of the most frequent n-gram and $N =\sum r * N_r$. Since N in such cases might be comparatively small, and $N_1$ comparatively large for authors with few short posts, unseen objects may have a unwarranted large probability\footnote{The value of 0.73 in \ref{Good-Turing} should not be taken as a representative case, as it was constructed as a pedagogical example} - and chosen more often since the shorter text must contain more unseen n-grams, than authors who have written long texts.\\

These results do not necessarily contract those found in \cite{nr4}, since \cite{nr4} worked with larger corpora from several well established authors. Such distributions are unlikely to be found on Internet forums or message board. 

\subsection{Ultimate tests}
The ultimate test clearly shows that authors with only 1 text (and thus most likely\footnote{Though there are some exceptions, A79-A82 being the most prominent - see \ref{reportTable}} a small part of the corpora) have far too many posts attributed to them, compared to authors who have written larger part of the corpora. Together with ShortBogusTets this is perhaps the most damming test, as it fails to correctly attribute any author who has written more than 10 posts - something an Authorship Attribution system for an Internet forum should be able to do.  

\subsection{Conclusion}
From the data found in the different tests, it is clear that the algorithm, or at least the implementation of the algorithm with Good-Turing smoothing, found in \cite{nr4} is not suitable for attributing authors in a corpora of 1329 forum posts\footnote{\pref{reportTable}}. I furthermore find it unlikely that a larger corpora would have given far better results, especially when it comes to tests like StressTest1, or UltimateTest. So while a larger corpora would likely give a more nuanced view, I doubt it would improve the results much. Thus, I have to conclude from these tests that the algorithm clearly is unsuited for the Authorship Attribution on for Internet forums.

\begin{itemize}
\item The StressTests shows that the algorithm is unable to identify an author from only one post (which would be needed if ``sock puppets'' are to be identified). 
\item ShortBogusTest shows that the algorithm attributes a far greater, and unfair, probability to authors who have written little
\item UltimateTest is the tests that mostly resemble a real world situation, and the fact that it fails, most likely due to the problems detected in ShortBogusTest and the initial smoke tests (\pref{smokeTest}), is a good indicator that the algorithm is not suited for Authorship Attribution on data from Internet forums.
\item AuthorSomePost likewise fails, which is disappointing, as there in this case should be enough information to find the correct author.
\item AuthorManyPost is a very interesting case, as its acceptable results are a stark departure from the previous results. It seems likely that the algorithm was designed to work under conditions like the ones provided in AuthorManyPost, given that \cite{nr4} tested their algorithm on well established authors.
\end{itemize}


