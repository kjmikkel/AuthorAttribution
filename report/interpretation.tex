\section{Interpretation}
I will in this section give my interpretation of the different test results presented in the last section. I will do so test by test, before giving a conclusion of all the tests.

\subsection{StressTest}
The test results in the last section clearly shows that all of the StressTests does not have a single hit.

\subsection{AuthorSomePost}
We see that in both 1 and 3 there is a single correct hit


\subsection{AuthorManyPost}
It is worth noting that here all 3 test have correct hits. While it would be fair to presume that this is because each author has more posts, it is worth nothing that the fact that both the first and second test has a far poorer score on all 4 dimensions (recall, precision, Overall Accuracy and Macro-average F-measure), than the third test, which has the fewest posts - see \ref{reportTable}, p. \pageref{reportTable}. From the same source it also becomes apperent that A1's post on average are shorter than A3's. This illustrates that more, and longer posts does not trivially make the algorithm better, but that the contents (and the n-grams found within), must play a large part in it.

\subsection{ShortBogusTest}
It is clear from the table that the algorithm consistently has chosen the bogus texts, which might be an indicator of that the algorithm weights corpora with few posts unfairly compared to those with longer, and that for it to work properly, there should be a more even distribution of posts from each author.\\

This is perhaps not so surprising, since \cite{nr4} worked with larger corpora from several well established authors. Such distributions cannot be guarantied on Internet forums or message boards (and will be especially useless if ``sock-puppets'' or anonymous posts are made) 

\subsection{Ultimate tests}
It is interesting to see that the only real test where the program excels is the ultimate tests, where one might think it would be more likely to fail (since the corpora contains more authors). One explanation might be that since the ultimate tests contain all the post made by an author, and hence more material to compare against, it can be more precise (especially if some of these authors had a more explicit writing style).

\subsection{Conclusion}
From the data found in the different tests, it is clear that the algorithm, or at least the implementation of the algorithm, found in \cite{nr4} is not suitable for attributing authors in a corpora of 1329 forum posts\footnote{See \ref{reportFile}}. Furthermore StressTest1 seems to indicate that it would be very hard to identify posts by ``sock puppets''\footnote{i.e. posts written by authors who register an extra account to either further their own causes or oppose others} or anonymous post, since it is unable to correctly identify the author of a singular post, even when the post itself appears in the corpora (and thus would have even greater difficulty if it was to match against its real author, who might have attempted to change his style while writing the post).\\ 

It should be mentioned, however, that most Internet forums or message boards have more than 1329 posts, and as such the results gained from this corpora might be slightly misleading.
