\section{Interpretation}
\label{interpretation}
I will in this section give my interpretation of the different test results presented in the last section. I will do so test by test, before giving a conclusion of all the tests.

\subsection{StressTest}
The test results in the last section clearly shows that all of the StressTests does not have a single hit. This is not very surprising - since we in each case only have one post, that has to be compared against the entire corpora.

\subsection{AuthorSomePost}
We see that in both 1 and 3 there is a single correct hit

\subsection{AuthorManyPost}
This test is an interesting case, since all of the subtests have correct hits, unlike all the others. While it would be fair to presume that this is because each author has more posts, it is worth nothing that the fact that both the first and second test has a far poorer score on all 4 dimensions (recall, precision, Overall Accuracy and Macro-average F-measure), than the third test, which has the fewest posts - \pref{reportTable}. From the same source it also becomes apparent that A1's post on average are shorter than A3's. This illustrates that more, and longer posts does not trivially make the algorithm better, but that the contents (and the n-grams found within), must play a large part in it.

\subsection{ShortBogusTest}
It is clear from the table that the algorithm consistently has chosen the bogus texts, which might be an indicator of that the algorithm weights corpora with few posts unfairly compared to those with longer, and that for it to work properly, there should be a more even distribution of posts from each author.\\

One possible explanation might lie in the way that Good-Turing calculates the value for all the unseen object (the n-grams that does not appear in the authors corpora). As previously stated in \ref{Good-Turing} (p. \pageref{Good-Turing}) the probability of an unseen object in the Good-Turing smoothing is $u =\frac{N_1}{N}$, where $N_1$ is the frequency of the frequency of the most frequent n-gram and $N =\sum r * N_r$. Since N might be comparatively small, and $N_1$ comparatively large for authors with few short posts u will be far larger than warranted\footnote{The value of 0.73 in \ref{Good-Turing} should, however, not be taken as a representative case, as it was constructed as a pedagogical example} - and chosen more often since the shorter text must contain more unseen n-grams, than authors who have written long texts.\\

This is perhaps not so surprising, since \cite{nr4} worked with larger corpora from several well established authors. Such distributions cannot be guarantied on Internet forums or message boards (and will be especially useless if ``sock-puppets'' or anonymous posts are made) 

\subsection{Ultimate tests}
The ultimate test clearly shows that authors with only 1 text (and thus most likely\footnote{Though there are some exceptions, A79-A82 being the most prominent - see \ref{reportTable}} a small part of the corpora) have far too many posts attributed to them, compared to authors who have written a larger part of the corpora. Together with ShortBogusTets this is perhaps the most damming test, as it fails to correctly attribute any author who has written more than 10 posts - something an Authorship Attribution algorithm for an Internet forum should be able to do.  

\subsection{Conclusion}
From the data found in the different tests, it is clear that the algorithm, or at least the implementation of the algorithm, found in \cite{nr4} is not suitable for attributing authors in a corpora of 1329 forum posts\footnote{\pref{reportTable}}. 
\begin{itemize}
\item The StressTests shows that the algorithm is unable to identify an author from only one post (which would be needed if ``sock puppets'' are to be identified). 
\item ShortBogusTest shows that the algorithm attributes a far greater, and unfair, probability to authors who have written little (i.e. those whose texts )
\item UltimateTest is the tests that mostly resemble a real world situation, and the fact that it pretty utterly fails, due to the problems detected in ShortBogusTest and the initial smoke tests (\pref{smokeTest}), is a good indicator that the algorithm is not suited for usage on Internet forums.
\item AuthorSomePost likewise returns bad results 
\item Interestingly AuthorManyPost shows far better results than all of the other test. Given the problems the other tests, possibly, have had with authors with few posts, this is perhaps not surprising, as these do not exist in this case.
\end{itemize}real-world 
Furthermore StressTest1 seems to indicate that it would be very hard to identify posts by ``sock puppets'' or anonymous post, since it is unable to correctly identify the author of a singular post, even when the post itself appears in the corpora (and thus would have even greater difficulty if it was to match against its real author, who might have attempted to change his style while writing the post).\\ 

Having said thus, it is important to remember that most Internet forums or message boards have more than 1329 posts, and therefore the real accuracy of the algorithm might be slightly off - due to the small size of the corpora. This does however not change the fact that the conclusion from these tests must be that the algorithm is clearly unsuited for the Authorship Attribution on for Internet forums.
